from pyspark.sql import SparkSession, functions as F
import json
import argparse

def iniciar_spark():
    return SparkSession.builder.appName("Analisis_Estres_Estudiantes").getOrCreate()

def cargar_datos(spark, ruta, formato="csv"):
    if formato == "csv":
        datos = spark.read.option("header", True).option("inferSchema", True).csv(ruta)
    elif formato == "json":
        datos = spark.read.json(ruta)
    else:
        datos = spark.read.parquet(ruta)
    return datos

def limpiar_datos(df):
        for c in df.columns:
        df = df.withColumnRenamed(c, c.lower().replace(" ", "_"))

    df = df.replace("", None).dropna(how="all").dropDuplicates()

    num_cols = [c for c, t in df.dtypes if t in ("double", "int")]
    for c in num_cols:
        prom = df.select(F.mean(F.col(c))).first()[0]
        if prom:
            df = df.na.fill({c: prom})

    if "stress_level" in df.columns:
        df = df.withColumn(
            "riesgo_estrés",
            F.when(F.col("stress_level") >= 8, "Alto")
             .when(F.col("stress_level") >= 5, "Medio")
             .otherwise("Bajo")
        )
    return df

def analisis_basico(df):
    print("----- Análisis Exploratorio -----")
    print("Número de filas:", df.count())
    print("Número de columnas:", len(df.columns))
    print("Columnas:", df.columns)
    print("Vista previa:")
    df.show(5)

    if "stress_level" in df.columns:
        df.groupBy("riesgo_estrés").count().show()
        df.groupBy("gender", "riesgo_estrés").count().show()

def operaciones_rdd(df):
    rdd = df.rdd
    print("----- Operaciones con RDD -----")
    print("Total de registros:", rdd.count())

    if "degree" in df.columns:
        conteo = rdd.map(lambda x: (x["degree"], 1)).reduceByKey(lambda a, b: a + b)
        print("Estudiantes por programa académico:")
        for x in conteo.collect():
            print(x)

def guardar(df, ruta, formato="parquet"):
    if formato == "csv":
        df.write.mode("overwrite").csv(ruta, header=True)
    elif formato == "json":
        df.write.mode("overwrite").json(ruta)
    else:
        df.write.mode("overwrite").parquet(ruta)

def main(args):
    spark = iniciar_spark()
    print("Cargando datos...")
    df = cargar_datos(spark, args.entrada, args.formato)

    print("Mostrando información inicial:")
    analisis_basico(df)

    print("Limpiando y procesando datos...")
    df_limpio = limpiar_datos(df)

    print("Datos limpios:")
    df_limpio.show(5)

    operaciones_rdd(df_limpio)

    print("Guardando resultados procesados...")
    guardar(df_limpio, args.salida, args.formato_salida)

    resumen = {
        "filas_iniciales": df.count(),
        "filas_finales": df_limpio.count(),
        "columnas": df_limpio.columns,
    }
    with open(args.salida.rstrip("/") + "/resumen.json", "w", encoding="utf-8") as f:
        json.dump(resumen, f, ensure_ascii=False, indent=2)

    print
    spark.stop()

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--entrada", required=True)
    parser.add_argument("--salida", required=True)
    parser.add_argument("--formato", default="csv")
    parser.add_argument("--formato_salida", default="parquet")
    args = parser.parse_args()
    main(args)

