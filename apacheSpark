#!/usr/bin/env python3
import argparse
import json
from pyspark.sql import SparkSession, DataFrame
from pyspark.sql import functions as F
from pyspark.sql.types import StructType, StructField, StringType, DoubleType, IntegerType, TimestampType

def crear_sesion_spark(nombre_app="AplicacionSparkBatch"):
    spark = SparkSession.builder.appName(nombre_app).getOrCreate()
    return spark

def cargar_datos(spark: SparkSession, ruta: str, formato: str = "csv", inferir_esquema: bool = True, encabezado: bool = True, esquema: StructType = None) -> DataFrame:
    formato = formato.lower()
    lector = spark.read
    if formato == "csv":
        lector = lector.option("header", str(encabezado).lower()).option("inferSchema", str(inferir_esquema).lower())
    if esquema is not None:
        df = lector.format(formato).schema(esquema).load(ruta)
    else:
        df = lector.format(formato).load(ruta)
    return df

def analisis_exploratorio(df: DataFrame, top_n: int = 10) -> dict:
    eda = {}
    eda["filas_totales"] = df.count()
    eda["columnas_totales"] = len(df.columns)
    eda["columnas"] = df.columns
    columnas_numericas = [f.name for f in df.schema.fields if isinstance(f.dataType, (DoubleType, IntegerType))]
    if columnas_numericas:
        desc = df.select(columnas_numericas).describe().toPandas().set_index("summary").to_dict()
        eda["estadisticas_numericas"] = desc
    else:
        eda["estadisticas_numericas"] = {}
    valores_nulos = {c: int(df.filter(F.col(c).isNull() | (F.col(c) == "")).count()) for c in df.columns}
    eda["valores_nulos"] = valores_nulos
    valores_top = {}
    for c in df.columns[:min(len(df.columns), 6)]:
        top = df.groupBy(c).count().orderBy(F.desc("count")).limit(top_n).collect()
        valores_top[c] = [(fila[0], fila[1]) for fila in top]
    eda["valores_top"] = valores_top
    return eda

def limpiar_y_transformar(df: DataFrame) -> DataFrame:
    for c in df.columns:
        df = df.withColumnRenamed(c, c.strip().lower().replace(" ", "_"))
    df = df.replace("", None)
    df = df.na.drop(how="all")
    df = df.dropDuplicates()
    columnas_numericas = [f.name for f in df.schema.fields if isinstance(f.dataType, (DoubleType, IntegerType))]
    for col in columnas_numericas:
        promedio = df.select(F.mean(F.col(col))).first()[0]
        if promedio is not None:
            df = df.na.fill({col: promedio})
    if set(["precio", "cantidad"]).issubset(set(df.columns)):
        df = df.withColumn("costo_total", F.col("precio") * F.col("cantidad"))
    return df

def operaciones_rdd(df: DataFrame) -> dict:
    resultados = {}
    rdd = df.rdd
    resultados["total_filas_rdd"] = rdd.count()
    if "categoria" in df.columns:
        conteo_categorias = rdd.map(lambda fila: (fila["categoria"], 1)).reduceByKey(lambda a, b: a + b).collect()
        resultados["conteo_categorias"] = sorted(conteo_categorias, key=lambda x: -x[1])
    else:
        resultados["conteo_categorias"] = []
    return resultados

def guardar_resultados(df: DataFrame, salida: str, modo: str = "overwrite", particion_por: list = None, formato: str = "parquet"):
    formato = formato.lower()
    escritor = df.write.mode(modo)
    if particion_por:
        escritor = escritor.partitionBy(particion_por)
    escritor.format(formato).save(salida)

def guardar_json(objeto: dict, ruta: str):
    with open(ruta, "w", encoding="utf-8") as f:
        json.dump(objeto, f, ensure_ascii=False, indent=2)

def main(args):
    spark = crear_sesion_spark("AplicacionSparkBatch")
    print(f"Cargando datos desde {args.entrada} (formato={args.formato}) ...")
    df = cargar_datos(spark, args.entrada, formato=args.formato, inferir_esquema=not args.sin_inferir, encabezado=not args.sin_encabezado)
    print("Esquema inicial:")
    df.printSchema()
    print("Primeras filas:")
    df.show(5, truncate=False)
    print("Realizando análisis exploratorio...")
    eda = analisis_exploratorio(df, top_n=10)
    print("Resumen EDA:", json.dumps({k: (v if k != "estadisticas_numericas" else "objeto estadísticas") for k, v in eda.items()}, indent=2, ensure_ascii=False))
    print("Limpiando y transformando datos...")
    df_limpio = limpiar_y_transformar(df)
    print("Esquema después de limpieza:")
    df_limpio.printSchema()
    print("Ejemplo de operaciones con RDD:")
    resultados_rdd = operaciones_rdd(df_limpio)
    print("Resultados RDD:", resultados_rdd)
    salida_procesada = args.salida.rstrip("/") + "/procesado"
    print(f"Guardando datos procesados en {salida_procesada} como {args.formato_salida} ...")
    guardar_resultados(df_limpio, salida_procesada, modo="overwrite", particion_por=None, formato=args.formato_salida)
    reporte = {
        "ruta_entrada": args.entrada,
        "filas_antes": eda["filas_totales"],
        "columnas_antes": eda["columnas_totales"],
        "valores_nulos_antes": eda["valores_nulos"],
        "filas_despues": df_limpio.count(),
        "columnas_despues": df_limpio.columns,
        "resumen_rdd": resultados_rdd
    }
    ruta_reporte = args.salida.rstrip("/") + "/reporte.json"
    print("Guardando reporte en", ruta_reporte)
    guardar_json(reporte, ruta_reporte)
    print("Ejecución finalizada.")
    spark.stop()

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Aplicación de procesamiento batch con Spark")
    parser.add_argument("--entrada", required=True, help="Ruta al archivo de entrada (local/hdfs/s3).")
    parser.add_argument("--formato", default="csv", choices=["csv", "parquet", "json"], help="Formato del archivo de entrada.")
    parser.add_argument("--salida", required=True, help="Directorio de salida para los resultados.")
    parser.add_argument("--formato-salida", default="parquet", choices=["parquet", "csv", "json"], help="Formato de salida.")
    parser.add_argument("--sin-inferir", action="store_true", help="Desactivar inferencia de esquema al leer CSV.")
    parser.add_argument("--sin-encabezado", action="store_true", help="Indicar que el CSV no tiene encabezado.")
    args = parser.parse_args()
    main(args)
